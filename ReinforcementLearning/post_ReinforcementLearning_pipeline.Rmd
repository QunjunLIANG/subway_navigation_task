---
title: "Post RF Analysis"
output:
  html_document:
    df_print: paged
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook to conduct the post-analysis of reinforcement learning pipeline. 

In preview attempts, I found an interesting R package - [ReinforcementLearning](https://cran.r-project.org/web/packages/ReinforcementLearning/vignettes/ReinforcementLearning.html), it could stimuli the environment and train an agent to go through in. Thus, I wrote a R pipeline to train a RF agent with subway navigation, a Demo below. The agent learns the action by a model-free learning, which known as Q-learning algorithm.

The RF training function looks like this:

```{r RF_funciton, message=FALSE}
library(tidyverse)
# function to export the journey chosen by RF agent
RFagentTrainingDemo <- function(actionsInd, jourEnv, startPos, goalPos) {
  stationOder <- c(paste0('a', 1:3), 'e1', paste0('a', 4:6), 
                   'e2', paste0('a', 7:8), paste0('b', 1:5), 'e3', 'b6', 'e4',
                   'b7', paste0('c', 1:6), 'e5', paste0('c' , 7:8), paste0('d', 1:7))
  if (!startPos%in%stationOder) {
    stop('illegal input station!')
  }
  library(ReinforcementLearning)
  takeAct <- actionsInd
  trainData <- sampleExperience(N = 1000, env = jourEnv,
                                states = stationOder, actions = takeAct)
  controlRf <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)
  rfModel <- ReinforcementLearning(data = trainData, 
                                   s = 'State', a = 'Action',
                                   r = 'Reward', s_new = 'NextState',
                                   iter = 10, control = controlRf)
  updateData <- sampleExperience(N = 500, env = jourEnv, states = stationOder,
                                 actions = takeAct, actionSelection = 'epsilon-greedy',
                                 model = rfModel, control = controlRf)
  updateModel <- ReinforcementLearning(updateData, s = 'State', a = 'Action',
                                       r = 'Reward', s_new = 'NextState',
                                       control = controlRf, model = rfModel)
  print('Agent training finished')
  outJour <- startPos
  thisStation <- startPos
  OptimalAction <- predict(updateModel, thisStation)
  Qvalue <- updateModel$Q[thisStation, OptimalAction]
  nextStep <- SubwayMap(state = thisStation, action = OptimalAction)
  outJour <- c(outJour, nextStep)
  while (nextStep != goalPos) {
    thisStation <- nextStep
    OptimalAction <- predict(updateModel, thisStation)
    Qvalue <- c(Qvalue, updateModel$Q[thisStation, OptimalAction])
    nextStep <- SubwayMap(state = thisStation, action = OptimalAction)
    outJour <- c(outJour, nextStep)
  }
  #return(outJour[-length(outJour)]) 
  return(list("jour"=outJour[-length(outJour)], 
              "value" =Qvalue[-length(outJour)])) 
}
# function to demean
Demean <- function(inVec) {
  outVec <- inVec - mean(inVec)
  return(outVec)
}
# function to indicate the whole map
SubwayMap <- function(state, action) {
  next_state <- state
  if (state == state('a1') && action == "right") {next_state <- state('a2')}
  if (state == state('a2') && action == "right") {next_state <- state('a3')}
  if (state == state('a3') && action == "down") {next_state <- state('e1')}
  if (state == state('e1') && action == "left") {next_state <- state('b1')}
  if (state == state('e1') && action == "down") {next_state <- state('a4')}
  if (state == state('a4') && action == "down") {next_state <- state('a5')}
  if (state == state('a5') && action == "down") {next_state <- state('a6')}
  if (state == state('a6') && action == "down") {next_state <- state('e2')}
  if (state == state('e2') && action == "left") {next_state <- state('d4')}
  if (state == state('e2') && action == "right") {next_state <- state('d5')}
  if (state == state('e2') && action == "down") {next_state <- state('a7')}
  if (state == state('a7') && action == "down") {next_state <- state('a8')}
  if (state == state('b1') && action == "left") {next_state <- state('b2')}
  if (state == state('b2') && action == "left") {next_state <- state('b3')}
  if (state == state('b3') && action == "left") {next_state <- state('b4')}
  if (state == state('b4') && action == "down") {next_state <- state('b5')}
  if (state == state('b5') && action == "down") {next_state <- state('e3')}
  if (state == state('e3') && action == "left") {next_state <- state('c3')}
  if (state == state('e3') && action == "right") {next_state <- state('c4')}
  if (state == state('e3') && action == "down") {next_state <- state('b6')}
  if (state == state('b6') && action == "down") {next_state <- state('e4')}
  if (state == state('e4') && action == "left") {next_state <- state('d2')}
  if (state == state('e4') && action == "right") {next_state <- state('d3')}
  if (state == state('e4') && action == "down") {next_state <- state('b7')}
  if (state == state('c1') && action == "right") {next_state <- state('c2')}
  if (state == state('c2') && action == "right") {next_state <- state('c3')}
  if (state == state('c3') && action == "right") {next_state <- state('e3')}
  if (state == state('c4') && action == "right") {next_state <- state('c5')}
  if (state == state('c5') && action == "down") {next_state <- state('c6')}
  if (state == state('c6') && action == "down") {next_state <- state('e5')}
  if (state == state('e5') && action == "left") {next_state <- state('d3')}
  if (state == state('e5') && action == "right") {next_state <- state('d4')}
  if (state == state('e5') && action == "down") {next_state <- state('c7')}
  if (state == state('c7') && action == "down") {next_state <- state('c8')}
  if (state == state('d1') && action == "right") {next_state <- state('d2')}
  if (state == state('d2') && action == "right") {next_state <- state('e4')}
  if (state == state('d3') && action == "right") {next_state <- state('e5')}
  if (state == state('d4') && action == "right") {next_state <- state('e2')}
  if (state == state('d5') && action == "down") {next_state <- state('d6')}
  if (state == state('d6') && action == "down") {next_state <- state('d7')}
  if (state == state('a2') && action == "left") {next_state <- state('a1')}
  if (state == state('a3') && action == "left") {next_state <- state('a2')}
  if (state == state('a4') && action == "up") {next_state <- state('e1')}
  if (state == state('a5') && action == "up") {next_state <- state('a4')}
  if (state == state('a6') && action == "up") {next_state <- state('a5')}
  if (state == state('e2') && action == "up") {next_state <- state('a6')}
  if (state == state('e1') && action == "up") {next_state <- state('a3')}
  if (state == state('a7') && action == "up") {next_state <- state('e2')}
  if (state == state('a8') && action == "up") {next_state <- state('a7')}
  if (state == state('b7') && action == "up") {next_state <- state('e4')}
  if (state == state('e4') && action == "up") {next_state <- state('b6')}
  if (state == state('b6') && action == "up") {next_state <- state('e3')}
  if (state == state('e3') && action == "up") {next_state <- state('b5')}
  if (state == state('b5') && action == "up") {next_state <- state('b4')}
  if (state == state('b4') && action == "right") {next_state <- state('b3')}
  if (state == state('b3') && action == "right") {next_state <- state('b2')}
  if (state == state('b2') && action == "right") {next_state <- state('b1')}
  if (state == state('b1') && action == "right") {next_state <- state('e1')}
  if (state == state('c8') && action == "up") {next_state <- state('c7')}
  if (state == state('c7') && action == "up") {next_state <- state('e5')}
  if (state == state('e5') && action == "up") {next_state <- state('c6')}
  if (state == state('c6') && action == "up") {next_state <- state('c5')}
  if (state == state('c5') && action == "left") {next_state <- state('c4')}
  if (state == state('c4') && action == "left") {next_state <- state('e3')}
  if (state == state('e3') && action == "left") {next_state <- state('c3')}
  if (state == state('c3') && action == "left") {next_state <- state('c2')}
  if (state == state('c2') && action == "left") {next_state <- state('c1')}
  if (state == state('d7') && action == "up") {next_state <- state('d6')}
  if (state == state('d6') && action == "up") {next_state <- state('d5')}
  if (state == state('d5') && action == "left") {next_state <- state('e2')}
  if (state == state('d4') && action == "left") {next_state <- state('e5')}
  if (state == state('d3') && action == "left") {next_state <- state('e4')}
  if (state == state('d2') && action == "left") {next_state <- state('d1')}
  out <- next_state
  return(out)
}
```

I encoded all stations into a universal format, as follows.

```{r fig.height=8, fig.width=10}
library(png)
imgSubwayMap <- readPNG('stationEncode.png')
rPlot <- nrow(imgSubwayMap)/ncol(imgSubwayMap) 
plot(c(0,1),c(0,rPlot),type = "n",xlab = "",ylab = "",asp=1)
rasterImage(imgSubwayMap,0,0,1,rPlot)
```

After that, the agent could navigate in the subway-map from any given start and goal station. In the following trunk, we input 'b2' as the start point while the station 'c2' is the destination.

```{r RF agent navigates Demo}
actionDemo <- c('up', 'down', 'right', 'left')
goalDemo <- 'c2'
startDemo <- 'b2'

env_demo <- function(state, action, goal=goalDemo) {
  next_state <- state
  if (state == state('a1') && action == "right") {next_state <- state('a2')}
  if (state == state('a2') && action == "right") {next_state <- state('a3')}
  if (state == state('a3') && action == "down") {next_state <- state('e1')}
  if (state == state('e1') && action == "left") {next_state <- state('b1')}
  if (state == state('e1') && action == "down") {next_state <- state('a4')}
  if (state == state('a4') && action == "down") {next_state <- state('a5')}
  if (state == state('a5') && action == "down") {next_state <- state('a6')}
  if (state == state('a6') && action == "down") {next_state <- state('e2')}
  if (state == state('e2') && action == "left") {next_state <- state('d4')}
  if (state == state('e2') && action == "right") {next_state <- state('d5')}
  if (state == state('e2') && action == "down") {next_state <- state('a7')}
  if (state == state('a7') && action == "down") {next_state <- state('a8')}
  if (state == state('b1') && action == "left") {next_state <- state('b2')}
  if (state == state('b2') && action == "left") {next_state <- state('b3')}
  if (state == state('b3') && action == "left") {next_state <- state('b4')}
  if (state == state('b4') && action == "down") {next_state <- state('b5')}
  if (state == state('b5') && action == "down") {next_state <- state('e3')}
  if (state == state('e3') && action == "left") {next_state <- state('c3')}
  if (state == state('e3') && action == "right") {next_state <- state('c4')}
  if (state == state('e3') && action == "down") {next_state <- state('b6')}
  if (state == state('b6') && action == "down") {next_state <- state('e4')}
  if (state == state('e4') && action == "left") {next_state <- state('d2')}
  if (state == state('e4') && action == "right") {next_state <- state('d3')}
  if (state == state('e4') && action == "down") {next_state <- state('b7')}
  if (state == state('c1') && action == "right") {next_state <- state('c2')}
  if (state == state('c2') && action == "right") {next_state <- state('c3')}
  if (state == state('c3') && action == "right") {next_state <- state('e3')}
  if (state == state('c4') && action == "right") {next_state <- state('c5')}
  if (state == state('c5') && action == "down") {next_state <- state('c6')}
  if (state == state('c6') && action == "down") {next_state <- state('e5')}
  if (state == state('e5') && action == "left") {next_state <- state('d3')}
  if (state == state('e5') && action == "right") {next_state <- state('d4')}
  if (state == state('e5') && action == "down") {next_state <- state('c7')}
  if (state == state('c7') && action == "down") {next_state <- state('c8')}
  if (state == state('d1') && action == "right") {next_state <- state('d2')}
  if (state == state('d2') && action == "right") {next_state <- state('e4')}
  if (state == state('d3') && action == "right") {next_state <- state('e5')}
  if (state == state('d4') && action == "right") {next_state <- state('e2')}
  if (state == state('d5') && action == "down") {next_state <- state('d6')}
  if (state == state('d6') && action == "down") {next_state <- state('d7')}
  if (state == state('a2') && action == "left") {next_state <- state('a1')}
  if (state == state('a3') && action == "left") {next_state <- state('a2')}
  if (state == state('a4') && action == "up") {next_state <- state('e1')}
  if (state == state('a5') && action == "up") {next_state <- state('a4')}
  if (state == state('a6') && action == "up") {next_state <- state('a5')}
  if (state == state('e2') && action == "up") {next_state <- state('a6')}
  if (state == state('e1') && action == "up") {next_state <- state('a3')}
  if (state == state('a7') && action == "up") {next_state <- state('e2')}
  if (state == state('a8') && action == "up") {next_state <- state('a7')}
  if (state == state('b7') && action == "up") {next_state <- state('e4')}
  if (state == state('e4') && action == "up") {next_state <- state('b6')}
  if (state == state('b6') && action == "up") {next_state <- state('e3')}
  if (state == state('e3') && action == "up") {next_state <- state('b5')}
  if (state == state('b5') && action == "up") {next_state <- state('b4')}
  if (state == state('b4') && action == "right") {next_state <- state('b3')}
  if (state == state('b3') && action == "right") {next_state <- state('b2')}
  if (state == state('b2') && action == "right") {next_state <- state('b1')}
  if (state == state('b1') && action == "right") {next_state <- state('e1')}
  if (state == state('c8') && action == "up") {next_state <- state('c7')}
  if (state == state('c7') && action == "up") {next_state <- state('e5')}
  if (state == state('e5') && action == "up") {next_state <- state('c6')}
  if (state == state('c6') && action == "up") {next_state <- state('c5')}
  if (state == state('c5') && action == "left") {next_state <- state('c4')}
  if (state == state('c4') && action == "left") {next_state <- state('e3')}
  if (state == state('e3') && action == "left") {next_state <- state('c3')}
  if (state == state('c3') && action == "left") {next_state <- state('c2')}
  if (state == state('c2') && action == "left") {next_state <- state('c1')}
  if (state == state('d7') && action == "up") {next_state <- state('d6')}
  if (state == state('d6') && action == "up") {next_state <- state('d5')}
  if (state == state('d5') && action == "left") {next_state <- state('e2')}
  if (state == state('d4') && action == "left") {next_state <- state('e5')}
  if (state == state('d3') && action == "left") {next_state <- state('e4')}
  if (state == state('d2') && action == "left") {next_state <- state('d1')}
  if (next_state == state(goal) && state != state(goal)) {
    reward <- 100
  }else{
    reward <- -1
  }
  out <- list(NextState = next_state, Reward = reward)
  return(out)
}

RFagentTrainingDemo(actionsInd = actionDemo,
                    jourEnv = env_demo,
                    startPos = startDemo,
                    goalPos = goalDemo)
```
Here, the agent outputs a route that it though as the most valuable based on Q value.


First, we need to initial two functions to load the data we need.

```{r FunctionInitiation, echo=TRUE}
LoadData <- function(sbjName, runName, dataName) {
  library(tidyverse)
  dataFile_tmp <- stringr::str_replace(dataName,
                                       pattern = 'xxxx', replacement = as.character(sbjName)) %>%
    stringr::str_replace(pattern = 'yyyy', replacement = as.character(runName)) %>%
    readr::read_csv() %>%
    filter(optJour == 1) %>%
    group_by(run, journey) %>%
    summarise(mean = mean(matchIndex)) %>%
    mutate(subject = paste0('sub-', as.character(sbjName)), .before=run) %>%
    mutate(match=if_else(mean==1, 'match', 'miss'))
  return(dataFile_tmp)
}

LoadMissData <- function(sbjName, runName, dataName) {
  library(tidyverse)
  dataFile_tmp <- stringr::str_replace(dataName,
                                       pattern = 'xxxx', replacement = as.character(sbjName)) %>%
    stringr::str_replace(pattern = 'yyyy', replacement = as.character(runName)) %>%
    readr::read_csv() %>%
    filter(optJour == 1 & matchIndex != 1)
  return(dataFile_tmp)
}
```

Now, we can load the raw data and shape them into a easy-look type.

```{r DataLoadingIn, echo=TRUE, message=FALSE}
sbjList <- readr::read_csv('name_list', col_names = F)$X1
dataFile <- 'RF_result/sub-xxxx_run-yyyy_RFcompare.csv'
runList <- c(1:4)
resultFrame <- c()
missFrame <- c()
for (sbjInd in sbjList) {
  resultFrame <- rbind(resultFrame, map_dfr(runList, LoadData, sbjName=sbjInd, dataName=dataFile))
  missFrame <- rbind(missFrame, map_dfr(runList, LoadMissData, sbjName=sbjInd, dataName=dataFile))
}
```

Second, let's see in what extend the route of RF agent overlap with out subjects choice.

```{r, message=FALSE}
library(ggplot2)
library(ggeasy)
library(ggstatsplot)
```

Use bar plot to visualization the overlap and null test.

```{r fig.height=10, fig.width=15}
ggbarstats(
  data = resultFrame,
  x = match,
  y = subject,
  palette='Set3',
  bf.message = F
) +
  coord_flip() +
  easy_change_legend(what = 'position', to = 'up')
```

From the above bar plot, we know that the journeys designed by RF agent are over 87% match with the real choice by subjects, and the X-square test gives a significant result. What we make sense about the overlap is the underlie learning process maybe reinforcement learning, however, what we don't know is the specific RF type - model-based or model-free, they have taken.

Let's have a look from another side - the mis-match journeys, whether we could find something interesting.

First, we check the station distribution in mismatch journeys between agent's and subjects'.

```{r}
# subjects' station distribution
table(missFrame$sbjRoute)
# agent's station distribution
table(missFrame$agentRoute)
```
surprisingly! The stations that lead to a mismatch come from the same list. Let's do a visualization to get a directly and closely insight.

```{r}
datF1 <- data.frame('source'='subject', "station"=table(missFrame$sbjRoute))
datF1$station.Var1 <- factor(datF1$station.Var1, levels = c("b5","b7","c3","c7",
                                                            "d2","d4","e3","e5",
                                                            "b6","e4","d3",
                                                            "c4","c5","c6"))
datF2 <- data.frame('source'='agent', "station"=table(missFrame$agentRoute))
datF2$station.Var1 <- factor(datF1$station.Var1, levels = c("b5","b7","c3","c7",
                                                            "d2","d4","e3","e5",
                                                            "b6","e4","d3",
                                                            "c4","c5","c6"))
ggbarstats(data = rbind(datF1, datF2),
           x = source,
           y = station.Var1,
           counts = station.Freq,
           label = 'counts',
           bf.message = F) +
  coord_flip()
```

Now we know that subjects liked the pathway of "b6-e4-d3", across two hierarchies. However, the agent preferred "c4-c5-c6", which without transferring. That result suggests that the reinforcement learning process taken by human in navigation maybe not a pure model-free type.

